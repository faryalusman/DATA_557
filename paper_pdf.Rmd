---
title: "paper_pdf"
output:
  pdf_document: default
  word_document: default
tables: yes 
---

```{r setup, include=FALSE}
library("ggplot2")
library("naniar")
library("MASS")
library("readxl")
library("sqldf")
library("xtable")
library("sqldf")
library("readr")
library("stargazer")
library("sandwich")
library("lmtest")
library("ggplot2")
library("lindia")
library("gridExtra")

prepared_data <- read_csv("../data/prepared_data.csv", 
    col_types = cols(CHILDPOVRATE15 = col_number(), 
        CONVSPTH09 = col_number(), FFRPTH09 = col_number(), 
        FSRPTH09 = col_number(), GROCPTH09 = col_number(), 
        LACCESS_CHILD10 = col_number(), LACCESS_POP10 = col_number(), 
        METRO13 = col_number(), MILK_PRICE10 = col_number(), 
        PCT_18YOUNGER10 = col_number(), PCT_FREE_LUNCH09 = col_number(), 
        PC_SNAPBEN10 = col_number(), PC_WIC_REDEMP08 = col_number(), 
        PERCHLDPOV10 = col_number(), PERPOV10 = col_number(), 
        RECFACPTH09 = col_number(), SPECSPTH09 = col_number(), 
        SUPERCPTH09 = col_number(), VEG_ACRESPTH07 = col_number()))
food = read.csv("~/data/q1_and_q2.csv")
census <- read_csv("../data/census.csv")
data <- sqldf::sqldf("SELECT * FROM prepared_data 
              JOIN census
              USING(FIPS)")
data <- data[-c(23,24)]

```

# Question 2: Are local food choices associated with the Obesity rates? 

	
### Are the food source variables associated with the obesity rates in 2008/09? 


### Are the food source variables associated with the obesity rates in 2013/14? 

The following are the assumptions of the two linear regressions we used:
Independence:
A common assumption across all inferential tests is that the observations in the sample are independent from each other. 

Normality:
We have county level observations of over 3000 rows. As this value is relatively large, the central limit theorem will take effect and the non-normality does not matter. We also checked the residual qq plot for normality.  

Constant Variance for obesity rate:
We assess the plot of residuals vs fitted values, and it shows clear evidence of non-constant variance. There is clearly a pattern of residuals that is undesirable to us. We used robust standard error to try to correct the non-constant variance problem.

```{r,echo=F,fig.height=6,fig.width=9,dev.args=list(pointsize=10)}
par(mfrow=c(1,2),mar=c(5,4,4,1))
fit2=lm(PCT_OBESE_ADULTS08~ GROCPTH09 +SUPERCPTH09 +CONVSPTH09  + SPECSPTH09 + WICSPTH08 +FFRPTH09 + FMRKTPTH09 + RECFACPTH09, data = food)
scatter.smooth(fit2$fitted.values,fit2$residuals,cex=0.5,col="gray")
qqnorm(fit$residuals)
qqline(fit$residuals)
```

```{r,echo=F,fig.height=6,fig.width=9,dev.args=list(pointsize=10)}
par(mfrow=c(1,2),mar=c(5,4,4,1))
fit=lm(PCT_OBESE_ADULTS13~ GROCPTH14 +SUPERCPTH14 +CONVSPTH14  + SPECSPTH14 + WICSPTH12 +FFRPTH14 + FMRKTPTH16 + RECFACPTH14 +FOOD_TAX14, data = food)
scatter.smooth(fit$fitted.values,fit$residuals,cex=0.5,col="gray")
qqnorm(fit$residuals)
qqline(fit$residuals)
```


# Question 3: Are there any group of factors that predict eligibility in the school free lunch program.
We are interested in predicting the eligibility of students for the Free Lunch Program. As we only have county-level child demographic data from the years 2009 - 2010, we will choose the Free Lunch Program eligibility data from 2009. There are lots of different families of variables that could determine eligibility. Some of these include:

  -  Assistance : How much access to resources do vulnerable groups in the county have? Are there resources like SNAP (Supplemental Nutritional Assistance Program) benefits or WIC (Women, Infants, Children) benefits available?
  -  Food source availability: How many stores and restaurants are in the county?
  -  Socio-economic: Does the county suffer from signficant adult and/or child poverty rates.

As the data is census level data, it does not seem to be completely randomly collected. Thus we are not aiming to make causal inferences from our analysis, rather understand the factors at play. 


## Exploratory Data Analysis 
Using multiple linear regression, we are interested in predicting the Percentage Eligibility of Students in the Free Lunch program (out of the total number of students attending school). The independent variables we are interested in are the following:

  - a
  - b
  - c
  - d
  - e
  - f
  - g
  - h
  - i 
  - j
  - k
  - l 



```{r Renaming data, include = FALSE}
names(data)[4:23] =
  c("Number of People with Low Access, 2010", "Number of children with Low Access, 2010", "Grocery stores per thousand", "Supercenters per thousand", "Convenience stores per thousand", "Specialized stores per thousand", "Fast food restaurants per thousand", "Full service restaurants per thousand", "% of students eligible for FLP", "Per capita WIC redemptions", "Per capita SNAP benefits", "Milk price (dollars)", "Acres of vegetables harvested per thousand", "Recreation facilities per thousand", "% of population younger than 18", "Persistent Poverty indicator", "Child Poverty rate", "Persistent Child Poverty Indicator", "Metropolitan Area indicator", "Population in 2010")
```

```{r dimensions before , include = FALSE}
dim(data) #3140
```

```{r, include = FALSE}
data <- data[!is.na(data$`% of students eligible for FLP`), ]
```

```{r dimensions after removing NA, include = FALSE}
dim(data) # 3065
```

```{r Creating variables, include = FALSE}
#data$Pop_2010 <- gsub(",","",data$Pop_2010)
#data$Pop_2010 <- as.integer(data$Pop_2010)
```

```{r,  include = FALSE}
data$`Milk price in cents` = data$`Milk price (dollars)` * 100

data$`Count Eligible for FLP` = (data$`% of students eligible for FLP`/100)*(data$`% of population younger than 18`/100)*data$`Population in 2010`
data$`Students eligible for FLP per thousand` = (data$`Count Eligible for FLP` * 1000)/ data$`Population in 2010`
data$`Low access adults per thousand` = (data$`Number of People with Low Access, 2010` * 1000) / data$`Population in 2010`
data$`Low access children per thousand` = (data$`Number of children with Low Access, 2010`* 1000) / data$`Population in 2010`

#data$`Students eligible for FLP per hundred thousand` = data$`Students eligible for FLP per thousand`*100000
```

### Data Preparation
Initially, there are 3140 observations. However, we decide to remove observations that contain null values for Percent eligibility for the Free Lunch Program, our dependent variable. This reduces our observations to 3065. Before proceeding to model fitting, we will do some initial data exploration of our variables. 

### Exploratory Data Analysis 
Below we have the histogram of percent eligibiltiy in the free lunch program. While it does not follow a Gaussian distribution exactly, it seems to follow it roughly enough that we can attempt the regression. 
```{r,  warning = FALSE, echo  = FALSE, message = FALSE, w}
# ggplot of school lunch eligibility in percent data 
ggplot2::ggplot(data = data, aes(x=`% of students eligible for FLP`))+
  geom_histogram(color="darkblue", fill="lightblue") + labs(title = "Histogram of percentage eligiblity in free lunch program", x = "Pct. Eligibility in program", y = "Frequency") + theme_classic()
```
We can see that the histogram is not exactly normal but not so far off that we cannot attempt the regression. 

We are also interested in another potential dependent variable: Count of children eligible for the free lunch program per hundred thousand. This is calculated by the authors by first finding out the total student population by multiplying the proportion of population aged under 18 years old with the population of the county (estimated using census methods) in 2018. Next, the number of students are multiplied by the Percent Eligibility for the Free Lunch Program to find the Count of children eligible for the Free Lunch Program. The final step is to divide this figure by the county population to get the Count of Children eligible for the Free Lunch Program per thousand.  One key assumption and thus future model limitation here is that in the absence of better data, we assume that the population under 18 is equal to the population of students of school going age. 

```{r, warning = FALSE, echo  = FALSE}
# ggplot of school lunch eligibility in percent data 
ggplot2::ggplot(data = data, aes(x=data$`Students eligible for FLP per thousand`))+
  geom_histogram(color="darkred", fill="pink") + labs(title = "Historam of count eligiblity in free lunch program per hundred thousand", x = "Count Eligibility in program", y = "Frequency") + theme_classic()
```

This also seems like highly skewed data, which is expected as there will be some outlier counties with more than the usual amount of resources dedicated to the program and thus having close to all of the school-going population eligible for the free lunch program. 

Based on the comparison of the two histograms, we decide to proceed with the first dependent and attempt a linear regression. Before we jump into regression we should analyze the variables and make sure there is not too much data missing. 

```{r, include = FALSE}
subset <- data[c(6:14, 16:17, 19:22, 24:28)]
```

```{r, echo = FALSE}
gg_miss_var(subset)
```
From the above we can see the variables of Milk price, Per capita WIC redemptions, Acres of vegetables harvested, and Per capita SNAP benefits all have some missing values. We will replace these missing values with medians of the variables (imputing via mean may not be a good choice because these variables will likely be skewed). 

```{r Imputing medians, include = FALSE}
med_PC  = median(data$`Per capita SNAP benefits`, na.rm = TRUE)
data$`Per capita SNAP benefits`[is.na(data$`Per capita SNAP benefits`)] <- med_PC


med_WC  = median(data$`Per capita WIC redemptions`, na.rm = TRUE)
data$`Per capita WIC redemptions`[is.na(data$`Per capita WIC redemptions`)] <- med_WC

med_VG  = median(data$`Acres of vegetables harvested per thousand`, na.rm = TRUE)
data$`Acres of vegetables harvested per thousand`[is.na(data$`Acres of vegetables harvested per thousand`)] <- med_VG

med_milk = median(data$`Milk price in cents`, na.rm = TRUE)
data$`Milk price in cents`[is.na(data$`Milk price in cents`)] <- med_milk

```

After accounting for the missing values, we also want to know which variables are closely related to Percent Eligibility of the Free Lunch Program. We can do this with a simple correlation heatmap.


```{r fig.height = 10, fig.width = 10, echo = FALSE, message = FALSE}
library(corrplot)
subset <- data[c(6:14, 16:17, 19:22, 24,26:28)]

M <- cor(subset)
corrplot(M, type = "full", method  = "color", addrect = 3, mar = c(0,0,0,0), tl.cex = 1, cl.cex = 1, tl.col = "black")

```
Thus we can see that we expect there to be a fairly strong positive correlation between Percent Eligibility of the Free Lunch Program with per capita WIC redemptions and SNAP benefits, as well all the poverty variables. Thus we expect these variables to be significant in our regressions. 


## Model Fitting: Linear Regression 
We will attempt a linear regression with Percent of Students eligible for the Free Lunch Program as the dependent variable and the above variables listed as the independent variables. 

```{r regression_one, include = FALSE}
subset_1 <- data[c(6:14, 16:17, 19:22, 24,27:28)]
linear_model_1 <- lm(subset_1$`% of students eligible for FLP`~ ., data = subset_1)
#summary(linear_model_1)
```


```{r, warning = FALSE, include = FALSE}
stargazer::stargazer(linear_model_1, single.row = FALSE, no.space = TRUE)

```

 However, we also have to keep in mind that we are not sure the model assumption of constant error variance will hold. Thus we will also report the robust standard error estimates of this model which provide unbiased standard error estimates. 
 

```{r, warning = FALSE, include = FALSE}
cov1  <- vcovHC(linear_model_1, type = "HC1")
robust_se <- sqrt(diag(cov1))
# Adjust F statistic 
wald_results <- waldtest(linear_model_1, vcov = cov1)

stargazer(linear_model_1, linear_model_1, type = "latex",
          se        = list(NULL, robust_se),
          omit.stat = "f",
          add.lines = list(c("F Statistic (df = 3; 360)", "12.879***", "7.73***")), no.space = TRUE)
```

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{`\% of students eligible for FLP`} \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 `Grocery stores per thousand` & $-$2.375$^{***}$ & $-$2.375$^{*}$ \\ 
  & (0.758) & (1.280) \\ 
  `Supercenters per thousand` & $-$3.443 & $-$3.443 \\ 
  & (7.516) & (7.090) \\ 
  `Convenience stores per thousand` & 1.696$^{***}$ & 1.696$^{**}$ \\ 
  & (0.533) & (0.710) \\ 
  `Specialized stores per thousand` & $-$2.975 & $-$2.975 \\ 
  & (2.070) & (2.202) \\ 
  `Fast food restaurants per thousand` & 0.556 & 0.556 \\ 
  & (0.568) & (0.721) \\ 
  `Full service restaurants per thousand` & $-$1.447$^{***}$ & $-$1.447$^{***}$ \\ 
  & (0.326) & (0.514) \\ 
  `Per capita WIC redemptions` & 0.120$^{***}$ & 0.120$^{***}$ \\ 
  & (0.021) & (0.035) \\ 
  `Per capita SNAP benefits` & 0.271$^{***}$ & 0.271$^{***}$ \\ 
  & (0.030) & (0.040) \\ 
  `Acres of vegetables harvested per thousand` & 0.002$^{***}$ & 0.002$^{***}$ \\ 
  & (0.001) & (0.001) \\ 
  `Recreation facilities per thousand` & $-$9.146$^{***}$ & $-$9.146$^{***}$ \\ 
  & (2.057) & (2.237) \\ 
  `Persistent Poverty indicator` & $-$0.101 & $-$0.101 \\ 
  & (0.635) & (0.856) \\ 
  `Child Poverty rate` & 1.025$^{***}$ & 1.025$^{***}$ \\ 
  & (0.033) & (0.044) \\ 
  `Persistent Child Poverty Indicator` & 2.898$^{***}$ & 2.898$^{***}$ \\ 
  & (0.520) & (0.552) \\ 
  `Metropolitan Area indicator` & $-$0.225 & $-$0.225 \\ 
  & (0.349) & (0.382) \\ 
  `Milk price in cents` & 0.064$^{***}$ & 0.064$^{***}$ \\ 
  & (0.013) & (0.012) \\ 
  `Low access adults per thousand` & $-$0.010$^{***}$ & $-$0.010$^{**}$ \\ 
  & (0.003) & (0.004) \\ 
  `Low access children per thousand` & 0.041$^{***}$ & 0.041$^{**}$ \\ 
  & (0.012) & (0.019) \\ 
  Constant & 4.498$^{***}$ & 4.498$^{***}$ \\ 
  & (1.379) & (1.357) \\ 
 \hline \\[-1.8ex] 
F Statistic (df = 3; 360) & 12.879*** & 7.73*** \\ 
Observations & 3,065 & 3,065 \\ 
R$^{2}$ & 0.760 & 0.760 \\ 
Adjusted R$^{2}$ & 0.758 & 0.758 \\ 
Residual Std. Error (df = 3047) & 7.976 & 7.976 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

Thus we can see that the following variables are significant using both the robust and the non-robust standard error option:
  - Grocery Stores per thousand (negatively associated)
  - Convenience stores per thousand (positively associated)
  - Full service restaurants per thousand (negatively associated)
  - Per capita WIC redemptions (positively associated)
  - Per capita SNAP benefits (positively associated)
  - Acres of vegetables harvested per thousand (positively associated)
  - Recreation facilities per thousand (negatively associated)
  - Child poverty rate (positively associated)
  - Persistent child poverty indicator (positively associated)
  - Milk price in cents (positively associated) 
  - Number of low access adults per thousand (negatively associated) 
  - Number of low access children per thousand (positively associated)

### Coefficient Interpretation 
As the model is a linear regression, interpretation is straightforward, with one unit increase in the independent variable corresponding with a (in this case) percent change in Percent Eligibility of Enrollment in the Free Lunch Program. The largest positive coefficient is of the Persistent Child Poverty indicator variable. Thus if a county faces persistent child poverty, the Percent Eligibility increases by 2.89 %. The largest negative coefficient is that of recreation facilities. If there is an increase of one recreation facility per thousand people, percent eligibility decreases by 9.14 %. This seems like a larger than usual effect, although one possible hypothesis is that areas with more recreation facilities may be richer and and thus have stricter eligibility requirements. However, this is just conjecture. 

Out of the poverty variables, only child poverty rate and persistent child poverty indicator significant, with the latter having a larger coefficient. This indicates that the general poverty of the area is more useful in predicting the percent eligibility enrollment than the exact childhood poverty rate.

Out of the store and restaurant availability variables, it seems that grocery stores and full service restaurants correspond with decreased eligiblity for the Free Lunch Program while the opposite is true for convenience stores. This may be because the former stores' abiity to provide nutrition to children may outweigh that of convenience stores. 

Most of our variables are significant with this linear regression, bringing the Multiple R-squared to a 75.96% and adjusted R squared not far behind at 75.82%.

### Model Evaulation: Hypothesis Test 
We want to test the hypothesis that the above model is more useful than a null model. We can let our null hypothesis be $$H_{0} : \text{Model with no independent variables fits data better than linear regression model}$$


reduced model in predicting the Percent Eligibility for the Free Lunch Program. We can let our null hypothesis be that 

```{r, include = FALSE}
null_model <- lm(subset_1$`% of students eligible for FLP` ~ 1)
fit_model <- anova(null_model, linear_model_1)
fit_model
```

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrr}
  \hline
 & Res.Df & RSS & Df & Sum of Sq & F & Pr($>$F) \\ 
  \hline
1 & 3064 & 806174.77 &  &  &  &  \\ 
  2 & 3047 & 193832.11 & 17 & 612342.67 & 566.23 & 0.0000 \\ 
   \hline
\end{tabular}
\end{table}

As the F statistic is large and the p value is smaller than our significance level of 0.05, we reject the null hypothesis. This means we find evidence that there is at least one predictor whose slope is not 0.

Suppose we want to conduct another hypothesis test, this time letting the null model be just the predictors for the stores, restaurants, and vegetable harvests. We want to test whether these variables are better than our full regression. 

\newpage

$$H_{0} : \text{Model with only store, restaurant, and vegetable variables is a better predictor than our full model}$$
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrr}
  \hline
 & Res.Df & RSS & Df & Sum of Sq & F & Pr($>$F) \\ 
  \hline
1 & 3058 & 681494.35 &  &  &  &  \\ 
  2 & 3047 & 193832.11 & 11 & 487662.24 & 696.90 & 0.0000 \\ 
   \hline
\end{tabular}
\end{table}

As the F statistic is large and the p value is smaller than our significance level of 0.05, we reject the null hypothesis. This means we find evidence that there is at least one predictor apart from the store and restaurant variables that makes our full regression a better predictor of Percent Eligibility of Enrollment. 

However, we must be careful with making too many hypotheses as our probability of reaching a false positive or our type 1 error rate increases for every additional hypothesis we make.


### Model Evaluation: Goodness of fit plots
It is important to analzye the assumptions of linear regression and see if our model meets them. 

```{r, echo = FALSE}
resid_fit<-function(model){
    p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
    p1<-p1+stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")
    p1<-p1+xlab("Fitted values")+ylab("Residuals")
    p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_classic()
    return(p1)
}   

res_v_fit = resid_fit(linear_model_1)
res_v_fit
```


```{r, echo = FALSE}
quant_plot <- gg_qqplot(linear_model_1, scale.factor = 1) + theme_classic()
quant_plot
```

While the residuals look as if they are largely following the assumption of constant variance of residuals, there are still some outliers for 
larger values where the predictions get more extreme. This shows an error with the model assumptions. In addition, the quantile-quantile plot's deviating tails indicate the presence of non-normality in error residuals as well. While the latter can be accepted if we consider our sample to be large enough, the former violates the validity of the model




# End of writeup
------------------------------------------------------------------------------------------------------
```{r, include = FALSE}
data_2 <- subset_1

z_transform <- function(x) {
  mean_ = mean(x, na.rm = TRUE)
  sd_ = sd(x, na.rm = TRUE)
  value = (x - mean_)/sd_
  return(value)
}

data_2$`Number of People with Low Access, 2010 standardised` <- lapply(data_2$`Low access adults per thousand`, z_transform)
data_2$`Number of children with Low Access, 2010 standardised` <- lapply(data_2$`Low access children per thousand`, z_transform)
data_2$`Grocery stores per thousand standardised` <- lapply(data_2$`Grocery stores per thousand`, z_transform)
data_2$`Supercenters per thousand standardised`     <- lapply(data_2$`Supercenters per thousand`, z_transform)
data_2$`Convenience stores per thousand standardised`    <- lapply(data_2$`Convenience stores per thousand`, z_transform)
data_2$`Specialized stores per thousand standardised`    <- lapply(data_2$`Specialized stores per thousand`, z_transform)
data_2$`Fast food restaurants per thousand standardised` <- lapply(data_2$`Fast food restaurants per thousand` , z_transform)
data_2$`Full service restaurants per thousand standardised`   <- lapply(data_2$`Full service restaurants per thousand` , z_transform)
data_2$`Per capita WIC redemptions standardised`  <- lapply(data_2$`Per capita WIC redemptions` , z_transform)
data_2$`Per capita SNAP benefits standardised`  <- lapply(data_2$`Per capita SNAP benefits`, z_transform)
data_2$`Acres of vegetables harvested per thousand standardised` <- lapply(data_2$`Acres of vegetables harvested per thousand`, z_transform)
data_2$`Recreation facilities per thousand standardised` <- lapply(data_2$`Recreation facilities per thousand`, z_transform)
data_2$`Child Poverty rate standardised` <- lapply(data_2$`Child Poverty rate`, z_transform)
data_2$`Milk price in cents standardised` <- lapply(data_2$`Milk price in cents`, z_transform)
```

```{r, include = FALSE}
#data_3 <- data_2[c(7, 19:32, 12, 14,15)]
#model_5 <- lm(data_3$`% of students eligible for FLP` ~. ,data = data_3)
#plot(model_5, which = c(1,2))

```


  
```{r, include = FALSE}
#reduced_model <- lm(`% of students eligible for FLP`~ `Supercenters per thousand`, data = subset_1)
  
  
  
reduced_model <- lm(subset_1$`% of students eligible for FLP` ~ subset_1$`Grocery stores per thousand` + subset_1$`Convenience stores per thousand` + subset_1$`Specialized stores per thousand` + subset_1$`Fast food restaurants per thousand` + subset_1$`Full service restaurants per thousand` + subset_1$`Acres of vegetables harvested per thousand`)



fit_model_2 <- anova(reduced_model, linear_model_1)
fit_model_2
```
```{r, include = FALSE}
print(xtable(fit_model_2), type = "latex")
```

  
  
```{r, include = FALSE}
subset_2 <- data[c(6:11, 13:14, 16:17, 19:22, 24, 26:28)]
```


```{r, echo = FALSE, include = FALSE, eval = FALSE}
subset_2 

capped_threshold = quantile(subset_2$`Students eligible for FLP per thousand`, 0.99)[1]
capped_threshold

subset_3 <- subset_2 
subset_3[subset_3$`Students eligible for FLP per thousand` >= capped_threshold, "Students eligible for FLP per thousand"] <- capped_threshold
#range(data_3$`Students eligible for FLP per thousand`)

```

```{r IGNORE, include = FALSE, eval = FALSE}
model_2 <- lm(`Students eligible for FLP per thousand` ~. , data = subset_2)
plot(model_2, which=1)
```



```{r IGNORE_1, include = FALSE, eval = FALSE}
d <- boxcox(`Students eligible for FLP per thousand` ~. , data = subset_2)
lambda <- d$x # lambda values
lik <- d$y # log likelihood values for SSE
bc <- cbind(lambda, lik) # combine lambda and lik
sorted_bc <- bc[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE
head(sorted_bc, n = 10)

model_4 <- lm(`Students eligible for FLP per thousand`^0.626 ~. , data = subset_2)
plot(model_4, which=1)
```

  
  
  
  
